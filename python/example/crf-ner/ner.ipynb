{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .config(\"spark.jar\", \"lib/sparknlp.jar\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\")\\\n",
    "    .config(\"spark.dirver.maxResultSize\", \"2g\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download CoNLL2003 dataset\n",
    "2. Save 3 files eng.train, eng.testa, eng.testa, into working dir ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "class Annotation:\n",
    "    def __init__(self, annotatorType, begin, end, metadata):\n",
    "        self.annotatorType = annotatorType\n",
    "        self.begin = begin\n",
    "        self.end = end\n",
    "        self.metadata = metadata\n",
    "\n",
    "        \n",
    "annotation_schema = StructType([\n",
    "    StructField(\"annotatorType\", StringType()),\n",
    "    StructField(\"begin\", IntegerType(), False),\n",
    "    StructField(\"end\", IntegerType(), False),\n",
    "    StructField(\"metadata\", MapType(StringType(), StringType()))\n",
    "])\n",
    "    \n",
    "\n",
    "\n",
    "def readDataset(file, spark, doc_column = \"text\", label_column = \"label\"):\n",
    "    result = []\n",
    "    doc = \"\"\n",
    "    labels = []\n",
    "\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            items = line.split(' ')\n",
    "            word = items[0]\n",
    "            if word == \"-DOCSTART-\":\n",
    "                result.append((doc, labels))\n",
    "                doc = \"\"\n",
    "                labels = []\n",
    "            elif len(items) == 1:\n",
    "                doc = doc + \"\\n\"\n",
    "            else:\n",
    "                if doc:\n",
    "                    doc = doc + \" \"\n",
    "\n",
    "                begin = len(doc)\n",
    "                doc = doc + word\n",
    "                end = len(doc) - 1\n",
    "                ner = items[3]\n",
    "                labels.append(Annotation(\"named_entity\", begin, end, {\"tag\": ner}))\n",
    "\n",
    "    if doc:\n",
    "        result.append((doc, labels))\n",
    "    \n",
    "    global annotation_schema\n",
    "    \n",
    "    schema =  StructType([\n",
    "      StructField(doc_column, StringType()),\n",
    "      StructField(label_column, ArrayType(annotation_schema))\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    return spark.createDataFrame(result, schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_model(file, spark):\n",
    "    print(\"Dataset Reading\")\n",
    "    \n",
    "    start = time.time()\n",
    "    dataset = readDataset(file, spark)\n",
    "    print(\"Done, {}\\n\".format(time.time() - start))\n",
    "\n",
    "    print(\"Start fitting\")\n",
    "    params = CrfParams(minEpochs=10, \n",
    "                    l2=1.0, \n",
    "                    verbose=2, \n",
    "                    randomSeed=0, \n",
    "                    lossEps=1e-3, \n",
    "                    c0=2250000)\n",
    "\n",
    "    documentAssembler = DocumentAssembler()\\\n",
    "      .setInputCol(\"text\")\\\n",
    "      .setOutputCol(\"document\")\n",
    "\n",
    "    sentenceDetector = SentenceDetectorModel()\\\n",
    "      .setInputCols([\"document\"])\\\n",
    "      .setOutputCol(\"sentence\")\n",
    "\n",
    "    tokenizer = RegexTokenizer()\\\n",
    "      .setInputCols([\"document\"])\\\n",
    "      .setOutputCol(\"token\")\n",
    "\n",
    "    posTagger = PerceptronApproach()\\\n",
    "      .setCorpusPath(\"../../../src/test/resources/anc-pos-corpus/\")\\\n",
    "      .setIterations(5)\\\n",
    "      .setInputCols([\"token\", \"document\"])\\\n",
    "      .setOutputCol(\"pos\")\n",
    "\n",
    "    nerTagger = CrfBasedNer()\\\n",
    "      .setCrfParams(params)\\\n",
    "      .setInputCols([\"sentence\", \"token\", \"pos\"])\\\n",
    "      .setLabelColumn(\"label\")\\\n",
    "      .setOutputCol(\"ner\")\n",
    "\n",
    "    pipeline = Pipeline()\\\n",
    "      .setStages([\n",
    "        documentAssembler,\n",
    "        sentenceDetector,\n",
    "        tokenizer,\n",
    "        posTagger,\n",
    "        nerTagger\n",
    "      ])\n",
    "\n",
    "    return pipeline.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, explode\n",
    "\n",
    "\n",
    "def get_dataset_for_analysis(file, model, spark):\n",
    "    print(\"Dataset Reading\")\n",
    "    \n",
    "    start = time.time()\n",
    "    dataset = readDataset(file, spark)\n",
    "    print(\"Done, {}\\n\".format(time.time() - start))\n",
    "    \n",
    "    predicted = model.transform(dataset)\n",
    "    \n",
    "    global annotation_schema\n",
    "    \n",
    "    zip_annotations = udf(\n",
    "      lambda x, y: list(zip(x, y)),\n",
    "      ArrayType(StructType([\n",
    "          StructField(\"predicted\", annotation_schema),\n",
    "          StructField(\"label\", annotation_schema)\n",
    "      ]))\n",
    "    )\n",
    "    \n",
    "    return predicted\\\n",
    "        .withColumn(\"result\", zip_annotations(\"ner\", \"label\"))\\\n",
    "        .select(explode(\"result\").alias(\"result\"))\\\n",
    "        .select(\n",
    "            col(\"result.predicted\").alias(\"predicted\"), \n",
    "            col(\"result.label\").alias(\"label\")\n",
    "        )\n",
    "        \n",
    "def printStat(label, correct, predicted, predictedCorrect):\n",
    "    prec = predictedCorrect / predicted if predicted > 0 else 0\n",
    "    rec = predictedCorrect / correct if correct > 0 else 0\n",
    "    f1 = (2*prec*rec)/(prec + rec) if prec + rec > 0 else 0\n",
    "    \n",
    "    print(\"{}\\t{}\\t{}\\t{}\".format(label, prec, rec, f1))\n",
    "        \n",
    "\n",
    "def test_dataset(file, model, spark):\n",
    "    started = time.time()\n",
    "\n",
    "    df = get_dataset_for_analysis(file, model, spark)\n",
    "    lines = df.collect()\n",
    "    lines = [\n",
    "        (\n",
    "            r[\"predicted\"][\"metadata\"][\"tag\"].strip(), \n",
    "            r[\"label\"][\"metadata\"][\"tag\"].strip()\n",
    "        ) for r in lines]\n",
    "    \n",
    "    \n",
    "    correct = {}\n",
    "    predicted = {}\n",
    "    predictedCorrect = {}\n",
    "    \n",
    "    for (lPredicted, lCorrect) in lines:\n",
    "        correct[lCorrect] = correct.get(lCorrect, 0) + 1\n",
    "        predicted[lPredicted] = predicted.get(lPredicted, 0) + 1\n",
    "\n",
    "        if lCorrect == lPredicted:\n",
    "            predictedCorrect[lPredicted] = predictedCorrect.get(lPredicted, 0) + 1\n",
    "    \n",
    "    print(\"time: {}\".format(time.time() - started))\n",
    "    \n",
    "    correct = { key: correct[key] for key in correct.keys() if key != 'O'}\n",
    "    predicted = { key: predicted[key] for key in predicted.keys() if key != 'O'}\n",
    "    predictedCorrect = { key: predictedCorrect[key] for key in predictedCorrect.keys() if key != 'O'}\n",
    "    \n",
    "    labels = set(list(correct.keys()) + list(predicted.keys()))\n",
    "    \n",
    "    print(\"label\\tprec\\trec\\tf1\")\n",
    "    totalCorrect = sum(correct.values())\n",
    "    totalPredicted = sum(predicted.values())\n",
    "    totalPredictedCorrect = sum(predictedCorrect.values())\n",
    "    \n",
    "    printStat(\"Total\", totalCorrect, totalPredicted, totalPredictedCorrect)\n",
    "    \n",
    "    for label in labels:\n",
    "        printStat(label, correct.get(label, 0), predicted.get(label, 0), predictedCorrect.get(label, 0))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "folder = '.'\n",
    "train_file = os.path.join(folder, \"eng.train\")\n",
    "test_file_a = os.path.join(folder, \"eng.testa\")\n",
    "test_file_b = os.path.join(folder, \"eng.testb\")\n",
    "\n",
    "model = train_model(train_file, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\\nQuality on training data\")\n",
    "test_dataset(train_file, model, spark)\n",
    "\n",
    "print(\"\\n\\nQuality on validation data\")\n",
    "test_dataset(test_file_a, model, spark)\n",
    "\n",
    "print(\"\\n\\nQuality on test data\")\n",
    "test_dataset(test_file_b, model, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = get_dataset_for_analysis(\"eng.testa\", model, spark)\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
