{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from builtins import range\n",
    "from timeit import default_timer\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "\n",
    "# Out of Box PySpark Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# KPSOI Imports\n",
    "#from sentimentanalysis.settings import KP_CSV_FILE, KP_DATA_COL, KP_TEST_COL, \\\n",
    "#                                       SPARK_NLP_PATH, DATA_DIR\n",
    "#sys.path.append(SPARK_NLP_PATH)\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "# John Snow Labs Imports\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.clinical.annotators import *\n",
    "from sparknlp.common import RegexRule\n",
    "from sparknlp.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5478 rows of data from file: /Users/Will/kpsoi/sentiment-analysis/data/Kaiser-Permanente-Review-Export-2017.csv\n",
      "Dropped 941 rows with null data.\n",
      "Loaded 3638 training and 899 testing rows of data.\n"
     ]
    }
   ],
   "source": [
    "# data = spark.read.csv(KP_CSV_FILE, header=True) # extra commas breaking schema\n",
    "# reference: https://github.com/databricks/spark-csv\n",
    "#data = spark.read.format('com.databricks.spark.csv') \\\n",
    "#            .options(header='true', inferschema='true', escape='\"') \\\n",
    "#            .load(KP_CSV_FILE)\n",
    "\n",
    "total_rows = data.count()\n",
    "print('Loaded %i rows of data from file: %s' % (total_rows, KP_CSV_FILE))\n",
    "\n",
    "data = data.na.drop(subset=[KP_DATA_COL])\n",
    "print('Dropped %s rows with null data.' % (total_rows - data.count()))\n",
    "\n",
    "data = data.withColumn('pyspark_id', F.monotonically_increasing_id())\n",
    "\n",
    "train = data.where(F.col(KP_TEST_COL).isNull())\n",
    "test = data.where(F.col(KP_TEST_COL).isNotNull())\n",
    "\n",
    "ntrain = train.count()\n",
    "ntest = test.count()\n",
    "\n",
    "print('Loaded %s training and %s testing rows of data.' % (ntrain, ntest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = train.select(KP_DATA_COL, 'rating') \\\n",
    "            .rdd.filter(lambda (c, r): float(r) == 1) \\\n",
    "            .map(lambda (c, r): c) \\\n",
    "            .collect()\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'bad_comments/onestars.txt'), 'w') as badfile:\n",
    "    for comment in ones:\n",
    "        badfile.write('{}\\n'.format(comment.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fives = train.select(KP_DATA_COL, 'rating') \\\n",
    "            .rdd.filter(lambda (c, r): float(r) == 5) \\\n",
    "            .map(lambda (c, r): c) \\\n",
    "            .collect()\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'good_comments/fivestars.txt'), 'w') as goodfile:\n",
    "    for comment in fives:\n",
    "        goodfile.write('{}\\n'.format(comment.encode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(KP_DATA_COL) \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = RegexTokenizer() \\\n",
    "    .setOutputCol('tokens')\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"tokens\"]) \\\n",
    "    .setOutputCol(\"normalized\")\n",
    "\n",
    "spell_checker = SpellChecker() \\\n",
    "    .setInputCols(['normalized']) \\\n",
    "    .setOutputCol('spelled_tokens')\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols(['spelled_tokens']) \\\n",
    "    .setOutputCol('sentence')\n",
    "\n",
    "pos_tagger = POSTagger() \\\n",
    "    .setInputCols(['spelled_tokens', 'sentence']) \\\n",
    "    .setOutputCol('pos') \\\n",
    "    .setCorpusPath(os.path.join(SPARK_NLP_PATH, '../src/test/resources/anc-pos-corpus'))\n",
    "    \n",
    "sentiment_detector = ViveknSentimentDetector() \\\n",
    "    .setInputCols(['spelled_tokens', 'sentence']) \\\n",
    "    .setOutputCol('sentiment_score') \\\n",
    "    .setPositiveSource(os.path.join(DATA_DIR, 'good_comments/fivestars.txt')) \\\n",
    "    .setNegativeSource(os.path.join(DATA_DIR, 'bad_comments/onestars.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    spell_checker,\n",
    "    sentence_detector,\n",
    "    pos_tagger,\n",
    "    sentiment_detector\n",
    "]\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "model = pipeline.fit(train)\n",
    "train = model.transform(train)\n",
    "test = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating accuracy for initial word lists model on 899 labeled rows.\n",
      "\n",
      "Viveken Sentiment Model Results\n",
      "Positive Accuracy: 74.0%\n",
      "Negative Accuracy 99.0%\n",
      "Overall Accuracy 82.31%\n",
      "Time: 1784.92652798 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the accuracy of our PySpark Model\n",
    "\n",
    "def get_score(sentiment_score):\n",
    "    return sentiment_score[0].asDict()['metadata']['sda']\n",
    "\n",
    "_ = default_timer()\n",
    "\n",
    "print('Calculating accuracy for initial word lists model on %s labeled rows.\\n' % ntest)\n",
    "\n",
    "results = test.select(KP_TEST_COL, 'sentiment_score')\n",
    "\n",
    "pos_rdd = results.rdd.filter(lambda (sa, sp): sa == '+')\n",
    "num_pos = float(pos_rdd.count())\n",
    "\n",
    "neg_rdd = results.rdd.filter(lambda (sa, sp): sa == '-')\n",
    "num_neg = float(neg_rdd.count())\n",
    "\n",
    "pos_correct = pos_rdd \\\n",
    "        .filter(lambda (sa, sp): get_score(sp).lower() == 'positive') \\\n",
    "        .count()\n",
    "\n",
    "neg_correct = neg_rdd \\\n",
    "        .filter(lambda (sa, sp): get_score(sp).lower() == 'negative') \\\n",
    "        .count()\n",
    "\n",
    "pos_accuracy = pos_correct / num_pos\n",
    "neg_accuracy = neg_correct / num_neg\n",
    "\n",
    "accuracy = (pos_correct + neg_correct) / float(ntest)\n",
    "\n",
    "print('Viveken Sentiment Model Results\\n'\n",
    "      'Positive Accuracy: %s%%\\nNegative Accuracy %s%%\\n'\n",
    "      'Overall Accuracy %s%%\\nTime: %s s\\n' % \\\n",
    "      (round(pos_accuracy * 100, 2), round(neg_accuracy * 100, 2),\n",
    "       round(accuracy * 100, 2), default_timer() - _))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
